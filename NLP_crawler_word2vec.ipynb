{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web crawler for collecting article \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# A function of collecting articles containing a specific company's name from 'fromd' to 'tod'\n",
    "# e.g. getArticles(\"네이버\", \"2020.01.01\", \"2020.01.31)\n",
    "def getArticles(companies, fromd, tod):\n",
    "    print(\"Starttime:\",datetime.datetime.now())\n",
    "    company = quote(companies)         #url encoding\n",
    "    dates = [fromd,tod]                #from to\n",
    "    fromdate = dates[0]\n",
    "    todate = dates[1]\n",
    "    fromdate = fromdate.split('.')     \n",
    "    todate = todate.split('.')\n",
    "    fromdate=datetime.datetime(int(fromdate[0]), int(fromdate[1]), int(fromdate[2]))\n",
    "    todate=datetime.datetime(int(todate[0]), int(todate[1]), int(todate[2]))\n",
    "    dt = datetime.timedelta(-1)\n",
    "\n",
    "    # iteration by day counts\n",
    "    while fromdate <= todate:\n",
    "        time.sleep(0.1)\n",
    "        date = fromdate\n",
    "        urldate = date.strftime('%Y.%m.%d')\n",
    "        url2 = \"https://search.naver.com/search.naver?&where=news&query=\"+company+\"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=\"+urldate+\"&de=\"+urldate+\"&docid=&nso=so:r,a:all&mynews=0&mson=0&refresh_start=0&related=0\"\n",
    "        requrl2 = req.Request(url2, headers = {'User-Agent':'Mozilla/5.0'})\n",
    "        res2 = req.urlopen(requrl2).read().decode('utf8')             #url decoding\n",
    "        soup2 = BeautifulSoup(res2,'html.parser')\n",
    "        \n",
    "        if not soup2.find('div',{'class':'title_desc'}) == None:\n",
    "            a = soup2.find('div',{'class':'title_desc'}).text        \n",
    "            maxContentNum = int(re.compile(r'/(.*)건$').search(a).group(1).split(' ')[-1].replace(',',''))  \n",
    "            maxPage = int(maxContentNum/10) + 1\n",
    "#            print(maxContentNum, maxPage)\n",
    "        \n",
    "        # List saving article chunk\n",
    "            press = []\n",
    "            title = []\n",
    "            content = []\n",
    "            link = []\n",
    "\n",
    "            for i in range(maxPage):\n",
    "                url = \"https://search.naver.com/search.naver?&where=news&query=\"+company+\"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=\"+urldate+\"&de=\"+urldate+\"&docid=&nso=so:r,a:all&mynews=0&cluster_rank=1001&start=\"+str(i*10 +1)+\"&refresh_start=0\"\n",
    "                requrl = req.Request(url, headers = {'User-Agent':'Mozilla/5.0'})\n",
    "                res = req.urlopen(requrl).read().decode('utf8')\n",
    "                soup = BeautifulSoup(res,'html.parser')\n",
    "                s=soup.find('ul',{'class':'type01'})\n",
    "                \n",
    "                if s != None:\n",
    "                    for inline in s.findAll('dd',{'class':'txt_inline'}):\n",
    "                        naver = inline.find('a')\n",
    "                          \n",
    "                # Designated one big search engine Naver, which is the biggest in Korea, to collect articles since HTML Tags vary across all newspapers\n",
    "                        if \"네이버뉴스\" == naver.text and \"전자신문\" != inline.find('span',{'class':'_sp_each_source'}).text:\n",
    "                            siteurl = naver.get('href')\n",
    "                            siteurl3 = req.Request(siteurl, headers = {'User-Agent':'Mozilla/5.0'})\n",
    "                            try3 = 0\n",
    "                            error_count = 0\n",
    "                            while try3 < 5:\n",
    "                                try:\n",
    "                                    res3 = req.urlopen(siteurl3)\n",
    "                                    try3 = 5\n",
    "                                except Exception:\n",
    "                                    try3 = try3 + 1\n",
    "                                    error_count = error_count + 1\n",
    "                                    print(try3)\n",
    "                                    time.sleep(5)\n",
    "                                    pass\n",
    "                                \n",
    "                            if error_count >= 5:\n",
    "                                raise NotImplementedError\n",
    "                            try:\n",
    "                                soup3 = BeautifulSoup(res3,'html.parser')\n",
    "                            except NotImplementedError:\n",
    "                                soup3 = BeautifulSoup(res3,'lxml')\n",
    "                            finally:\n",
    "                            \n",
    "                                try:\n",
    "                                    tit = soup3.find('div',{'class':'article_info'}).find('h3').text\n",
    "                                    cont = soup3.find('div',{'id':'articleBodyContents'}).text.split('{}')[-1]\n",
    "                            \n",
    "                                    if tit.upper().count(companies.upper()) > 0 or cont.upper().count(companies.upper()) > 0:\n",
    "                                        link.append(siteurl)\n",
    "                                        press.append(inline.find('span',{'class':'_sp_each_source'}).text.strip())\n",
    "                                        title.append(tit.strip())\n",
    "                                        content.append(cont.strip())\n",
    "                                except AttributeError:\n",
    "                                    pass\n",
    "\n",
    "#         data = {\"press\":press, \"link\":link, \"title\":title,\"content\":content}\n",
    "#         data = pd.DataFrame(data)\n",
    "#         data.to_csv(companies+'_'+urldate.replace('.','')+'.csv',encoding=None)\n",
    "#         return data.head()\n",
    "            # Save collected data\n",
    "            path = \"D:/projects/articles/\"+companies\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            if (len(press)==len(title))&(len(content)==len(press))&(len(press)==len(link))&(len(press)*len(content)*len(link)*len(title)!=0):\n",
    "                with open(path+'/'+companies+'_'+urldate.replace('.','')+'.csv', 'w',newline='') as outputFile:\n",
    "                    outputWriter = csv.writer(outputFile)\n",
    "            # Remove characters arising CP949 encoding problems\n",
    "                    for p in range(len(press)):\n",
    "                        try:\n",
    "                            outputWriter.writerow([link[p].replace('\\xa0','').replace('\\xa9','').replace('\\u301c','').replace('\\u5d1b','').replace('\\u22c5','').replace('\\u7f51','').strip(),\n",
    "                                                   title[p].replace('\\xa0','').replace('\\xa9','').replace('\\u301c','').replace('\\u5d1b','').replace('\\u22c5','').replace('\\u7f51','').strip(),\n",
    "                                                   press[p].replace('\\xa0','').replace('\\xa9','').replace('\\u301c','').replace('\\u5d1b','').replace('\\u22c5','').replace('\\u7f51','').strip(),\n",
    "                                                   content[p].replace('\\xa0','').replace('\\xa9','').replace('\\u301c','').replace('\\u5d1b','').replace('\\u22c5','').replace('\\u7f51','').strip()])\n",
    "                        except UnicodeEncodeError:\n",
    "                            pass\n",
    "        \n",
    "        fromdate = fromdate - dt\n",
    "\n",
    "        print(\"Endtime: \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding words into vectors to get similarit features\n",
    "# Get articles for a company from directory\n",
    "import os, csv\n",
    "from konlpy.tag import Twitter\n",
    "import konlpy\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def getContents(path):\n",
    "    files = os.listdir(path)\n",
    "    file1 = [file for file in files]\n",
    "\n",
    "    contents=[]\n",
    "    for filename in file1:\n",
    "        exampleFile = open(path + filename)\n",
    "        exampleReader = csv.reader(exampleFile)\n",
    "        for row in exampleReader:\n",
    "            contents.append(row[3])\n",
    "    return contents\n",
    "\n",
    "path = \"D:/projects/articles/하이닉스/\"\n",
    "train_data = getContents(path)\n",
    "\n",
    "# Tokenize textual data\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in Twitter().pos(doc, norm=True, stem=True)] \n",
    "            \n",
    "train_docs = [row for row in train_data]\n",
    "sentences = [tokenize(d) for d in train_docs]\n",
    "\n",
    "# Embed words into vectors\n",
    "def selectNoun(w):\n",
    "    ww = []\n",
    "    for words in w:\n",
    "        splited = words.split('/')\n",
    "        if(splited[-1]=='Noun') and (len(splited[0])>=2):\n",
    "            ww.append(words)\n",
    "    return ww\n",
    "sentences = [selectNoun(s) for s in sentences]\n",
    "model = word2vec.Word2Vec(sentences)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Example of result with a word '우려'\n",
    "model.most_similar(['우려/Noun'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
